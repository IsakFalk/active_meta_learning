#+TITLE: Notes / Lab Book
#+AUTHOR: Isak Falk
#+EMAIL: ucabitf@ucl.ac.uk
#+DATE: \today
#+DESCRIPTION: Lab book of thoughts and notes (this is free form and also functions as a kind of diary)
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:5 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:not-in-toc
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LATEX_HEADER: \usepackage{macros}
#+LATEX_HEADER: \usepackage{mathtools}


* NEXT What is Meta Learning
:LOGBOOK:
- State "NEXT"       from "NEXT"       [2019-10-10 Thu 17:50]
:END:
Before we can answer if there is any active learning to be done, we need to
understand what meta learning actually is. Below is a short list of questions we
should answer in the [[./literature_review.org][lit review]]
- What is meta learning
  - [[file:~/life/references/bibliography/pdfs/vilalta02_persp_view_survey_meta_learn.pdf][A Perspective View and Survey of Meta-Learning]]
  - [[file:~/life/references/bibliography/pdfs/finn17_model.pdf][MAML for deep learning]]
    - Multi-task learning
      - [[file:~/life/references/bibliography/pdfs/caruana97_multit_learn.pdf][Multi-Task learning]]
      - [[file:~/life/references/bibliography/pdfs/maurer13_spars.pdf][Sparse coding for multitask and transfer learning]]
      - [[file:~/life/references/bibliography/pdfs/ciliberto17_consis.pdf][Consistent Multitask learning with nonlinear output]]
      - [[file:~/life/references/bibliography/pdfs/ciliberto15_convex.pdf][Convex Learning of multiple tasks and their structure]]
      - [[file:~/life/references/bibliography/pdfs/ruder17_overv_multi_task_learn_deep_neural_networ.pdf][An Overview of Multi-Task Learning in Deep NNs]]
      - [[file:~/life/references/bibliography/pdfs/argyriou07_multi.pdf][Multi-Task Feature learning]]
      - [[file:~/life/references/bibliography/pdfs/zhang17_survey_multi_task_learn.pdf][A survey on Multi-Task Learning]]
    - Transfer learning
      - [[file:~/life/references/bibliography/pdfs/pan09_survey_trans_learn.pdf][A survey on Transfer Learning]]
      - [[file:~/life/references/bibliography/pdfs/raina07_self.pdf][Self-taught learning: Transfer Learning from unlabeled data]]
      - [[file:~/life/references/bibliography/pdfs/pan08_trans.pdf][Transfer learning via Dimensionality Reduction]]
  - Neigbouring fields
    - [[file:~/life/references/bibliography/pdfs/quionero-candela09_datas.pdf][Dataset Shift]]
    - Learning-to-learn
      - [[file:~/life/references/bibliography/pdfs/denevi18_increm_learn_to_learn_with_statis_guaran.pdf][Incremental Learning to Learn with statistical guarantees]]
      - [[file:~/life/references/bibliography/pdfs/denevi18_learn.pdf][Learning to learn around a common mean]]
    - Lifelong learning
    - Curriculum learning
- How can it be formulated
  - SLT
  - Other
- Equivalent problems in other fields
  - Statistics
  - Control
  - ML
  - Signal Processing
  - Operations Research
  - etc.
- Current approaches to Active Meta-Learning

* DONE Current Meta Learning setup
CLOSED: [2019-10-22 Tue 15:13]
:LOGBOOK:
- State "DONE"       from "NEXT"       [2019-10-22 Tue 15:13]
- State "NEXT"       from              [2019-10-15 Tue 17:23]
:END:
Me and Carlo has brain spawned a bit and thought about ways to go about this. We
will attack this from a SLT point of view.

** Setup
We have a meta distribution \(\rho_{\mu}\) over some space of distributions
\(\D_{\Zc}\) such that \(\rho_{\Zc} \sim \rho_{\mu}\), where \(\Zc\) is our
sample space. Normally in supervised learning we have that \(\Zc = \X \times
\Y\). For each sampled distribution \(\rho_{\Zc}\) we sample a dataset (which we
will call /task/ to stay consistent with literature) \(\task =
(z_{i})_{i=1}^{n} \sim
\rho_{\Zc}^{n}\) iid, which is split up into \(n^{tr}, n^{val}\) sized dataset such
that \(\task = D^{tr} \cup D^{val} = (z_{i})_{i=1}^{n^{tr}} \cup
(z_{j})_{j=n^{tr}+1}^{n^{tr} + n^{val}}\) where \(D^{tr}, D^{val}\) is the train
and validation dataset respectively.

Now assume the following problem, we have \(m\) distributions
\((\rho_{i})_{i=1}^{m} \sim \rho_{\mu}^m\) sampled iid. Each of these
distributions \(\rho_{i}\) gives rise to a task \(\task_{i} = D_{i}^{tr} \cup
D_{i}^{val}\), where we make the simplifying assumption that \(n_{i} =
n^{tr}_{i} + n^{val}_{i}\) is the same for any \(i\), so that \(n^{tr}_{i} =
n^{tr}, n^{val}_{i} = n^{val}\) and \(n_{i} = n^{tr} + n^{val}\).

Our problem looks as follows, we want to find an algorithm
\(\malgo{\varphi}{\cdot}: \Zc^{\ast} \to \Hc\) where \(\Zc^{\ast}\) is the set of
all possible datasets of any size and \(\Hc\) is our hypothesis space, we let
\(\varphi\) denote the /hyperparameters/ (we will also write \(\varphi = \task_{I}\)
where \(I\) is some index set, e.g. \(I = \upto{k}\) to show that we have run
the meta-learning algorithm on \((\task_{1}, \dots, \task_{k})\)) of the algorithm which
we want to learn, as we are doing meta-learning rather than normal supervised
learning. In this sense we may learn \(\varphi\) from the outer loop, and the
algorithm then uses these to adjust its behaviour when mapping from the train
set to a target function in the hypothesis class. We assume that \(\Hc\) is the
same for all distributions, such that \(f_{i} \in \Hc\) for all \(i\), but
\(f_{i}\) differ in general.

(Rewriting of previous paragraph, we can view this as a three step process as
follows:
1. The hyper-meta algorithm takes sets of tasks \((\task_{i})_{i=1}^{m}\) and
   maps to algorithms that are
2. Meta algorithms that map from training sets \(D^{tr}\) to supervised
   algorithms that
3. Maps from training sets \(D^{tr}\) to a hypothesis set
Not sure if this is a fruitful way to look at it, but at least it clarifies the
different levels we are operating on here. One way to write this (sloppily)
could be as follows, if \(T^{\ast}\) is the set of all sequences of tasks, we
call \(\mathcal{M}_{1}\) the hyper-meta, \(\mathcal{M}_{0}\) the meta, and
\(\algo\) the base algorithm, then we have the following nested way of coupling
them
#+begin_export latex
\begin{equation*}
\mathcal{M}_1: T^{\ast} \to \{\mathcal{M}_0: \Zc^{\ast} \to \{ \algo: \Zc^{\ast} \to \Hc \} \}
\end{equation*}
#+end_export
)

We introduce the following notation, the loss function is a function
#+begin_export latex
\begin{equation*}
\ell: \Zc \times \Hc \to \R_{+}
\end{equation*}
#+end_export
which takes as input a sample point \(z \in \Zc\) and a hypothesis \(h \in \Hc\)
getting a loss of choosing this hypothesis for this sample point, \(\ell(h,
z)\). Note that this way of writing the loss may be highly non-linear as can be
seen in the following case when \(z = (x, y)\), \(\ell(z, h) = (h(x) - y)^{2}\).
The actual loss can now be written in the following form, for a task \(\task =
D^{tr} \cup D^{val}\), we have that the loss of the algorithm \(\malgo{\phi}{\cdot}\) is
#+begin_export latex
\begin{equation*}
L(\malgo{\phi}{\cdot}, \task) = \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \ell(\malgo{\phi}{D^{tr}}, z).
\end{equation*}
#+end_export

From this we can formulate the learning problem for meta learning. Given the
above, we want to understand how to perform well on the /meta-risk/
#+begin_export latex
\begin{equation*}
\err{\rho_{\mu}}{\malgo{\phi}{\cdot}} = \E_{\rho \sim \rho_{\mu}}\left[ \E_{\task \sim \rho^{n}} \left[L(\malgo{\phi}{\cdot}), \task) \right] \right].
\end{equation*}
#+end_export
It's clear that we can actually rewrite this further, since relying on the iid
assumption we have that
#+begin_export latex
\begin{align*}
  \E_{\task \sim \rho^{n}} \left[L(\malgo{\phi}{\cdot}), \task) \right] &= \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{D^{val} \sim \rho^{n_{val}}} \left[ \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \ell(\malgo{\phi}{D^{tr}}, z) \right] \right] \\
  & = \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{z \sim \rho} \left[ \ell(\malgo{\phi}{D^{tr}}, z) \right] \right]
\end{align*}
#+end_export
from which we get that the meta-risk can be expressed as
#+begin_export latex
\begin{align*}
  \err{\rho_{\mu}}{\malgo{\phi}{\cdot}} &= \E_{\rho \sim \rho_{\mu}} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{z \sim \rho} \left[ \ell(\malgo{\phi}{D^{tr}}, z) \right] \right]  \right]\\
                                        &= \E_{\rho \sim \rho_{\mu}} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \err{\rho}{\malgo{\phi}{D^{tr}}} \right] \right]
\end{align*}
#+end_export

A way to do this is to do well in probability over the train set which is what
is usually done in SLT. Explicitly, assuming as above that we have a set of distributions
(which we will call /base/ distributions, where base correspond to the base
level in contrast to /meta/ which corresponds to the meta level)
\((\rho_{i})_{i=1}^{m} \sim \rho_{\mu}^{m}\) iid, and each \(\rho_{i}\) gives
rise to a task \(\task_{i}\) sampled iid, then we are interested in bounds of
the form
#+begin_export latex
\begin{equation*}
\Pr_{(\task_i)_{i=1}^m}(\err{\rho_{\mu}}{\malgo{\phi}{\cdot}} - \err{\rho_{\mu}}{\algo_{\ast}} \geq \epsilon) \leq \delta,
\end{equation*}
#+end_export
where \(\algo_{\ast} = \inf_{\algo} \err{\rho_{\mu}}{\algo}\). We probably want
to constrain this in the future, but leave this like this for now.

** Approach to solving this
The general problem is hard to solve, instead we consider how the generalisation
error for an algorithm behaves. Consider the following expression (which
differs from the one above but taken from photos of what Carlo wrote on screen),
we assume that we have \(m\) different training tasks \(M = (\task_{i})_{i=1}^m\)
and will use the shorthand \(\task_{1:m}\) to mean all the tasks in index set.
For an active learning algorithm on a meta-level, for each \(t \leq m\) we let
\(M_{t}\) be a subset of tasks of size \(t\), \(M_{t} \subseteq M, \abs{M_{t}} =
t\). We are interested in quantifying the following
#+begin_export latex
\begin{equation*}
  \Pr_{M} \left( \E_{\task \sim \rho_T}[L(\malgo{M}{\cdot}, \task) - L(\malgo{M_t}{\cdot}, \task)] \geq \epsilon \right) \leq \delta
\end{equation*}
#+end_export

We make the following additional assumptions
- The base loss \(\ell(f(x), y)\) is Lipschitz with respect to the second argument with
  constant \(L\).
- The meta-algorithm \(\malgo{\phi}{\cdot}\) exists in some vector-valued
  reproducing kernel hilbert space cite:alvarez12_kernel_vector_valued_funct. In
  particular this means the following (following cite:ciliberto16), there is
  some vvRKHS \(\Gc\) consisting of functions mapping from \(\X \to \Hc\)
  where \(\Hc\) is some separable Hilbert space, we will assume that \(\Hc
  \subseteq \R^{d}\) since instances of datapoints normally comes in column
  form.

The definition of an vvRKHS is a generalisation of the univariate case. In
particular the vvRKHS \(\G\) is characterised by a so called /kernel of positive
type/ which is an operator values bi-linear map \(\Gamma: \X \times \X \to
B(\Hc, \Hc)\). Since we assume that \(\Hc\) is a subspace of Euclidean space,
\(\Gamma\) will map to positive semi-definite matrices. The vvRKHS is built in a
similar way to the univariate case with first a pre-Hilbert space which gets
completed by adding the limit points, with the inner product
#+begin_export latex
\begin{equation*}
\scal{\Gamma(x, \cdot))c}{\Gamma(x', \cdot)c'}_{\G} = \scal{\Gamma(x, x')c}{c'}_{\Hc}
\end{equation*}
#+end_export
which leads to the reproducing property, for any \(x \in \X, c \in \Hc\) and \(g
\in \G\), we have that
#+begin_export latex
\begin{equation*}
\scal{g(x)}{c}_{\Hc} = \scal{g}{\Gamma(x, \cdot)}{c}_{\G}
\end{equation*}
#+end_export
and that for each \(x \in \X\), the function \(\Gamma(x, \cdot): \G \to \Hc\) is
the evaluation function in \(x\) on \(\G\), that is \(\Gamma(x, \cdot)(g) =
g(x)\) and \(\Gamma(x, \cdot) \in \G\).
  
Consider now the expression in the expectation, the expected deviation of the
meta-loss between the meta-learning algorithm trained on the full dataset and
the subset of taska, we can write this as follows
#+begin_export latex
\begin{align*}
\abs{L(\malgo{M}{\task}) - L(\malgo{M_{t}}{\task})} &\leq \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \abs{\ell(\malgo{M}{D^{tr}}, z) - \ell(\malgo{M_{t}}{D^{tr}}, z)} \\
                                                    &= \frac{L}{\abs{D^{val}}}\sum_{x \in D^{val}} \abs{\malgo{M}{D^{tr}}(x) - \malgo{M_{t}}{D^{tr}}(x)} \\
\end{align*}
#+end_export

Now in order to proceed we need to think about how we can decouple \(D^{tr}\)
and \(M\) or \(M_{t}\). If we let \(\malgo{M}{\cdot} \in \Hc_{1} \otimes
\Hc_{2}\) and both \(\Hc_{1}, \Hc_{2}\) are RKHS's then we have that \(\Hc_{1} \otimes
\Hc_{2}\) is also an RKHS. Consider now
#+begin_export latex
\begin{align*}
\abs{\malgo{M}{D^{tr}}(x) - \malgo{M_{t}}{D^{tr}}(x)} &= \abs{\scal{\malgo{M}{D^{tr}} - \malgo{M_{t}}{D^{tr}}}{K_{\Hc_2}(x, \cdot)}}\\
                                                      &\leq \norm{\malgo{M}{D^{tr}} - \malgo{M_{t}}{D^{tr}}}_{\Hc_2}\norm{K_{\Hc_2}(x, \cdot)}_{\Hc_2}\\
                                                      &= \norm{\scal{\malgo{M}{\cdot} - \malgo{M_{t}}{\cdot}}{\mu_{\Hc_2}(D^{tr})}}_{\Hc_2}\norm{K_{\Hc_2}(x, \cdot)}_{\Hc_2}\\
\end{align*}
#+end_export

* Meetings
** Meeting (Carlo) <2019-10-22 Tue>

Me and Carlo went over the setting and did some slight changes to notation. In
general I have to make sure that I don't overload the characters I use for sets,
distributions and so on (easier said than done given the amount of stuff I use).

*** MMD for noiseless supervised learning

We first broke down the actual problem, taking inspiration from what made the
MMD bound possible in the MRes dissertation I wrote, we looked at what caused
the bound (which only holds in the noiseless case)
#+begin_export latex
\begin{equation*}
  \abs{\err{P}{h} - \err{Q}{h}} \leq \MMD{P}{Q}{\Hc} + \eta_{MMD}
\end{equation*}
#+end_export
where we can control \(\eta_{MMD}\) by making careful choices about the
regression and MMD RKHSs and how they relate to each other.

*** MMD for meta-learning
Writing the above out explicitly, we have that for the supervised learning case
that
#+begin_export latex
\begin{equation*}
  \abs{\err{P}{h} - \err{Q}{h}} = \abs{\frac{1}{n_{P}}\sum_{i=1}^{n_{P}}\ell(h, z_{i}) - \frac{1}{n_{Q}}\sum_{j=1}^{n_{Q}}\ell(h, z_{j})}
\end{equation*}
#+end_export

If we now consider our case, we can write this as follows
#+begin_export latex
\begin{equation*}
  \abs{\err{P}{\metalg} - \err{Q}{\metalg}} = \abs{\frac{1}{n_{P}}\sum_{i=1}^{n_{P}}L(\metalg, D^{tr}_{i}, D^{val}_{i}) - \frac{1}{n_{Q}}\sum_{j=1}^{n_{Q}}L(\metalg, D^{tr}_{j}, D^{val}_{j})}
\end{equation*}
#+end_export
and each of the losses \(L(\metalg, D^{tr}, D^{val})\) are defined as follows
#+begin_export latex
\begin{equation*}
L(\metalg, D^{tr}, D^{val}) = \frac{1}{\abs{D^{val}}} \sum_{z \in D^{val}} \ell(\metalg(D^{tr}), z).
\end{equation*}
#+end_export

We proceed to make the following assumption, \(\ell(h, z) =
\scal{\psi(h)}{\phi(z)}_{\Gc}\) which is reminisent of the restriction in
cite:ciliberto16 on the loss function. This leads to the expression of the
meta-loss as
#+begin_export latex
\begin{equation*}
L(\metalg, D^{tr}, D^{val}) = \scal{\psi(\metalg(D^{tr}))}{\frac{1}{\abs{D^{val}}} \sum_{z \in D^{val}} \phi(z)}_{\Gc} = \scal{\psi(\metalg(D^{tr}))}{\mu(D^{val})}_{\Gc}
\end{equation*}
#+end_export
where we use the feature map \(\phi\) on \(\Zc\) to some RKHS \(\Gc\) to induce
a feature map on /sets/ (or more generally distributions) through the mean
embedding \(\mu(D^{val})\). This leads to a mean embedding of mean embedding
(which should be the same as having a /mixture distribution/, which is what we
get over the actual supervised learning \(z\)'s, where the tasks act as mixtures
we draw \(z\) from. This leads to the following
#+BEGIN_EXPORT latex
\begin{align*}
  \abs{\frac{1}{n_P} \sum_{i=1}^{n_P}
  \scal{\psi(\metalg(D^{tr}_i))}{\mu(D^{val}_i)} - \frac{1}{n_Q}
  \sum_{j=1}^{n_Q} \scal{\psi(\metalg(D^{tr}_j))}{\mu(D^{val}_j)}}
  &=
\end{align*}
#+END_EXPORT

This does not seem too good, but maybe we can do something similar to
what we did in the MMD proof.

*** Revisiting MMD proof
Let's go through the proof in my dissertation /
cite:viering17_nuclear_discr_activ_learn and see if we can extract
assumptions to make this work. Without any assumptions, for arbitrary
\(A, B\) this holds
#+BEGIN_export latex
\begin{align*}
  \abs{\err{P}{\metalg} - \err{Q}{\metalg}} &= \abs{\frac{1}{n_{P}}\sum_{i=1}^{n_{P}}L(\metalg, D^{tr}_{i}, D^{val}_{i}) - \frac{1}{n_{Q}}\sum_{j=1}^{n_{Q}}L(\metalg, D^{tr}_{j}, D^{val}_{j})}\\
                              &= \abs{\err{P}{\metalg} - A + A - B + B - \err{Q}{\metalg}}\\
                              &\leq \abs{\err{P}{\metalg} - A} + \abs{B - A} + \abs{\err{Q}{\metalg} - B}.
\end{align*}
#+END_export
Note that we can also

** Meeting (Carlo) <2019-11-04 Mon 17:00>
:PROPERTIES:
:EXPORT_FILE_NAME: 20191104_meeting_carlo
:END:
*** Preparation
Recap: Our setting is that of /meta-learning/ where we have a meta
distribution \(\mu\) over distributions with support on \(\Zc = \X
\times \Y\), our data-space. We have a base loss function \(\ell : \Zc
\times \Hc \to \R_{+}\) where \(\Hc\) is a hypothesis class. We sample
\(m\) iid distributions from \(\mu\) giving us
\((\rho_{i})_{i=1}^{m} \sim \mu^{m}\) and for each \(i \in \upto{m}\) we get an iid
sample of \(n\) datapoints, \(\task_{i} = (z_{j})_{j=1}^{n} \sim
\rho_{i}^{n}\).

For each dataset \(\task_{i}\) we split this into a train and test
set, \(\task_{i} = D^{tr}_{i} \cup D^{val}_{i}\) with \(n_{tr}\) and
\(n_{val}\) datapoints respectively. We define the meta-loss for an
algorithm \(\metalg : \Zc^{\ast} \to \Hc\) as
#+BEGIN_EXPORT latex
\begin{align*}
L(\metalg, \task) := L(\metalg, D^{tr}, D^{val}) &= \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \ell(\metalg(D^{tr}), z) \\
&= \err{D^{val}}{\metalg(D^{tr})}
\end{align*}
#+END_EXPORT
and we are interested in how to find good meta-algorithms that has low
meta-risk
#+BEGIN_EXPORT latex
\begin{align*}
  \err{\mu}{\metalg} &= \E_{\rho \sim \mu}\left[ \E_{\task \sim \rho^{n}} \left[L(\metalg, \task) \right] \right]\\
&= \E_{\rho \sim \mu} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{z \sim \rho} \left[ \ell(\metalg(D^{tr}), z) \right] \right]  \right]\\
&= \E_{\rho \sim \mu} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \err{\rho}{\metalg(D^{tr})} \right] \right].
\end{align*}
#+END_EXPORT

We do not have access to \(\mu\) but only have knowledge of the
meta-distribution implicitly through the datasets \(\task_{i} \sim
\rho_{i}^{n}\).

*** MMD work
As we are looking to do active learning, we are interested in bounds
inspired by the supervised learning bound for the noiseless case of
#+BEGIN_EXPORT latex
\begin{equation*}
\abs{\err{P}{h} - \err{Q}{h}} \leq \MMD{P}{Q}{\Hc'} + \eta_{MMD}
\end{equation*}
#+END_EXPORT
so we consider the same starting point. Let \(M =
(\task_{i})_{i=1}^{m}\) be the full meta-dataset and let \(M_{t}\) be
a subset of these tasks of size \(t\). Now for an arbitrary algorithm
\(\metalg\) consider
#+BEGIN_EXPORT latex
\begin{equation*}
\abs{\err{M}{\metalg} - \err{M_{t}}{\metalg}} = \abs{\frac{1}{m}\sum_{i=1}^{m}L(\metalg, D^{tr}_{i}, D^{val}_{i}) - \frac{1}{t}\sum_{j=1}^{t}L(\metalg, D^{tr}_{j}, D^{val}_{j})}.
\end{equation*}
#+END_EXPORT

The bound achieved for MMD in a noiseless setting is dependent on the
following assumptions
- \(\ell\) is any loss
- For arbitrary \(h \in H \subseteq \Hc\)
- Any train set \(S \subseteq P_{0}\) (here \(P_{0}\) is the
  equivalent of \(M\))
- With realizable case of \(f \in \Hc\)
- No noise; Distribution \(\rho\) determined by \(\rho_{\X}\)
- For any \(H' \subseteq \Hc\)
then
#+BEGIN_EXPORT latex
\begin{equation*}
 \err{P_{0}}{h} \leq \err{S}{h} + \MMD{P_{0}^{x}}{S^{x}}{H'} + \eta_{MMD}
\end{equation*}
#+END_EXPORT
where \(\eta_{MMD} = 2\min_{\tilde{g} \in H'} \max_{h \in H, x \in
P_{0}^{x}}\abs{\ell(f(x), h(x)) - \tilde{g}(x)}\). We can make this
work for our setting pretty easily by considering the following
decomposition
#+BEGIN_EXPORT latex
\begin{align*}
\abs{\err{M}{\metalg} - \err{M_{t}}{\metalg}} &= \abs{\frac{1}{m}\sum_{i=1}^{m}L(\metalg, \task_{i}) - \frac{1}{t}\sum_{j=1}^{t}L(\metalg, \task_{i})} \\
&\leq \abs{\err{M}{\metalg} - \frac{1}{m}\sum_{i=1}^{m} g(\task_{i})} + \abs{\frac{1}{m}\sum_{i=1}^{m} g(\task_{i}) - \frac{1}{t}\sum_{i=1}^{t} g(\task_{j})} + \abs{\err{M_{t}}{\metalg} - \frac{1}{t}\sum_{i=1}^{t} g(\task_{j})}
\end{align*}
#+END_EXPORT
If we assume that we have some RKHS suitably defined, which we call
\(\Gc\) and let \(g \in G \subseteq \Gc\) then since \(M_{t} \subseteq M\)
we have that the middle term \(\abs{\frac{1}{m}\sum_{i=1}^{m}
g(\task_{i}) - \frac{1}{t}\sum_{i=1}^{t} g(\task_{j})} \leq
\MMD{M}{M_{t}}{G}\) and the other two terms are less than \(\sup_{\task
\in M}\abs{L(\metalg, \task) - g(\task)}\) that
#+BEGIN_EXPORT latex
\begin{equation*}
\abs{\err{M}{\metalg} - \err{M_{t}}{\metalg}} \leq \MMD{M}{M_{t}}{G} + \inf_{g \in G}\sup_{\task
\in M}\abs{L(\metalg, \task) - g(\task)}
\end{equation*}
#+END_EXPORT

*** Notes from meeting
- [X] Clean up notation, don't use for example \(M, \mathcal{M}\) at
  the same time

**** MMD bound
Given the bound
#+BEGIN_EXPORT latex
\begin{equation*}
\abs{\err{M}{\metalg} - \err{M_{t}}{\metalg}} \leq \MMD{M}{M_{t}}{G} + \inf_{g \in G}\sup_{\task
\in M}\abs{L(\metalg, \task) - g(\task)}
\end{equation*}
#+END_EXPORT
we can control the MMD term by choosing \(\Gc\) in a proper way. We
note that this will have to be an RKHS which maps from \(2^{\Zc}\) to
\(\R\). However this is slightly complicated by the second
term, \(\inf_{g \in G}\sup_{\task \in M}\abs{L(\metalg, \task) - g(\task)}\). We
need to choose \(\Gc\) so that this term disappears, this means that
we need \(\Gc\) to be expressive enough to make this small (note to
self, we assumed that the function was in a ball in order to get rid
of this since \(G \subseteq \Gc\) trades of how big the MMD terms get
by a multiplicative factor (\(R\), the radius) and how enlargening
this \(R\) makes the second term grow smaller (we want to find the
optimal tradeoff essentially).

**** Finding a good \(\Gc\)
In order to investigate this, we'd have to look at what kind of
functions \(L(\metalg, \task)\) encode for various different
algorithms \(\metalg\) so that we can choose a \(\Gc\) that contain
functions approximating \(L(\metalg, \task)\) well, due to the term
#+BEGIN_EXPORT latex
\begin{equation*}
\inf_{g \in G}\sup_{\task \in M}\abs{L(\metalg, \task) - g(\task)}
\end{equation*}
#+END_EXPORT

Since this will depend on how we choose \(\metalg\) we will focus on
when \(\metalg\) is such that \(L(\metalg, \task)\) is a smooth
function of \(\task\) which means that we need to know how
#+BEGIN_EXPORT latex
\begin{align*}
L(\metalg, \task) &= L(\metalg, D^{tr}, D^{val})\\
&= \frac{1}{n_{val}}\sum_{i=1}^{n_{val}} \ell(\metalg(D^{tr}), z_{i})\\
&= \frac{1}{n_{val}}\sum_{i=1}^{n_{val}} \ell(\metalg(D^{tr})(x_{i}), y_{i})
\end{align*}
#+END_EXPORT
It's easy to see that \(L\) is built from the following parts
- \(S(\vb{a}) = \frac{1}{n_{val}}\sum_{i=1}^{n_{val}}a_{i} = \frac{1}{n_{val}}\ones^{T}\vb{a}\)
- \(\ell(y', y)\)
- \(\metalg(D^{tr})\)
- \(\metalg(D^{tr})(x)\)
and we can write this in the form of \(L = (S \circ
\ell)(\metalg(D^{tr})(\vb{X}), \vb{Y})\) where
#+BEGIN_EXPORT latex
\begin{align*}
  \metalg(D^{tr})(\vb{X}) &=
                            \begin{bmatrix}
                              \metalg(D^{tr})(x_{1}) \\
                              \vdots \\
                              \metalg(D^{tr})(x_{n_{tr}})
                            \end{bmatrix} \\
  \ell(\metalg(D^{tr})(\vb{X}), \vb{Y}) &=
                   \begin{bmatrix}
                     \ell(\metalg(D^{tr})(x_1), y_1) \\
                     \vdots \\
                     \ell(\metalg(D^{tr})(x_{n_{tr}}), y_{n_{tr}})
                   \end{bmatrix}
\end{align*}
#+END_EXPORT

The functional class of this will depend on how we choose \(\metalg\).
Below is an outline of what goes into a usual meta-algorithm.

- Inner loop :: We choose an algorithm that maps from a train set
                \(D^{tr}\) to a hypothesis space \(\Hc\). This will be
                done by minimizing the regularised ERM objective,
                \(\rerr{D^{tr}}{\lambda}{h} = \frac{1}{n_{tr}}\sum_{i=1}^{n^{tr}}\ell(h(x_{i}),
                y_{i}) + \lambda \norm{h}^{2}_{\Hc}\) where we will
                let the optimisation be (S)GD (just call this SGD from
                here on) with early stopping.
- Outer loop :: For example MAML cite:finn17_model, we will let this
                be for now and investigate this later. We would then
                like to know how the fine tuning factors in to this.

We can choose various architectures, which corresponds to the
hypothesis space (given a starting point \(h_{0}\) when doing SGD)
- MLP parametrised by \((n_{i}, n_{o}, L, \sigma)\) which are in turn
  the dimensions of the input, output, number of hidden layers and the
  non-linearity. As long as \(\sigma\) is differentiable this function
  is differentiable.
- KRR parametrised by \(K\), the kernel. Ditto for this.

**** KRR Least Squares solution
Assume that in the inner loop we are doing KRR, which means the
algorithm is the solution to the RERM problem over an RKHS \(\Hc\)
with some kernel \(K\) that is smooth in both its arguments
#+BEGIN_EXPORT latex
\begin{equation*}
\metalg(D^{tr}) = \argmin_{h \in \Hc}\frac{1}{n_{tr}}\sum_{i=1}^{n_{tr}}(h(x_{i}) - y_i)^{2} + \lambda \norm{h}^{2}_{\Hc}
\end{equation*}
#+END_EXPORT
which if we write \((\vb{K}_{x})_{i} = K(x_{i}, x)\) gives us the solution
#+BEGIN_EXPORT latex
\begin{equation*}
\metalg(D^{tr})(x) = \sum_{i=1}^{n_{tr}}\alpha_{i}K(x_{i}, x) = \vb{K}_{x}^{T}\vb{\alpha} = \vb{K}_{x}^{T}(\vb{K} + n_{tr}\lambda I_{n_{tr}})^{-1}\vb{Y}
\end{equation*}
#+END_EXPORT
and it's clear that
- \(\alpha(D^{tr}) = (\vb{K} + \lambda n_{tr}I)^{-1}\vb{Y}\) is smooth
  as a function of \(D^{tr}\)
    - \(K\) is smooth in both its arguments which means that
      \(\vb{K}\) is smooth as a function of \(\vb{X}\)
    - The matrix \(\vb{K} + n_{tr} \lambda I\) has smallest eigenvalue
      bounded away from zero so the inverse is also smooth as a function
      of \(\vb{X}\). Note that we fix the size of the dataset which
      means that \(n_{tr}\) doesn't vary.
- \(\sum_{i=1}^{n}\alpha_{i}K(x_{i}, x) = \vb{K}_{x}^{T}\vb{\alpha}\)
  is smooth as a function of \(\vb{X}\) and \(x\) for fixed
  \(\vb{\alpha}\) as it's a linear combination of smooth functions.
- Together we have that this is a smooth function of both \(D^{tr}\)
  and \(x\) which means that \(L(\metalg, \task)\) is a smooth
  function of \(\task\) when \(\metalg\) is KRR.

**** Early Stopping
As a first step, assume that we use KRR as our architecture, which
means that \(\Hc\) is an RKHS and we choose a gaussian kernel as the
kernel of this space. We then have that the solution to the RERM
problem is simply
#+BEGIN_EXPORT latex
\begin{equation*}
\metalg(D^{tr})(x) = \sum_{i=1}^{n_{tr}}\alpha_{i}K(x_{i}, x) = \vb{K}_{x}^{T}\vb{\alpha}
\end{equation*}
#+END_EXPORT
where \(\vb{\alpha} = (\vb{K} + n_{tr}\lambda I_{n_{tr}})^{-1}\vb{Y}\). We
see that if \(K(\cdot, \cdot)\) is smooth, then \(\metalg(D^{tr})(x)\)
is smooth in both \(x\) (sum of smooth functions) and \(D^{tr}\)
(The function \(g(\vb{X}) = (\vb{K} + n_{tr}\lambda I_{n_{tr}})^{-1}\)
is smooth everywhere as \(\vb{K}\) is a smooth and positive
semi-definite function of \(\vb{X}\) and so \(\vb{K} + n_{tr}\lambda
I_{n_{tr}} \succeq n_{tr}\lambda I_{n_{tr}}\) where \(n_{tr}\lambda > 0\).

How does early stopping change this? Something to note is that if we
let \(\lambda = 0\) then the inverse term is not smooth since it is
undefined when \(\vb{K}\) is has some eigenvalue equal to zero.

** Meeting (Massi) <2019-11-08 Fri 11:00>
:PROPERTIES:
:EXPORT_FILE_NAME: ./meetings/20191108_meeting_massi
:END:

*** Talk
Me and Massi got up to date on what me and Carlo have been working on,
debriefing him on the meta active learning setup with MMD. We talked
through the points we needed to clarify:

- Isolate the differences in active learning in meta learning to that
  of active learning from supervised learning.
- Better understand \(L_{\metalg}(\task) \coloneqq L(\metalg, \task)\)
  as a function of \(\task\).

Beside this, he recommended some papers I should look into
- cite:gupta17_pac_approac_to_applic_specif_algor_selec On
  Lipschitzness of meta-loss (not super relevant)
- cite:pentina17_multi Active / Semi-supervised learning (relevant)

A quick note: Massi seemed pessimistic if this was relevant to
pursuit, as the active learning bound for SL did not improve upon
uniform sampling. *I should bring this up with Carlo and Massi*.

* Bibliography
bibliography:/home/isak/life/references/bibliography/references.bib
bibliographystyle:unsrt
