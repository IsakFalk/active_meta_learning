#+TITLE: Notes / Lab Book
#+AUTHOR: Isak Falk
#+EMAIL: ucabitf@ucl.ac.uk
#+DATE: \today
#+DESCRIPTION: Lab book of thoughts and notes (this is free form and also functions as a kind of diary)
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:5 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:not-in-toc
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [bigger]
#+LATEX_HEADER: \usepackage{macros}

* NEXT What is Meta Learning
:LOGBOOK:
- State "NEXT"       from "NEXT"       [2019-10-10 Thu 17:50]
:END:
Before we can answer if there is any active learning to be done, we need to
understand what meta learning actually is. Below is a short list of questions we
should answer in the [[./literature_review.org][lit review]]
- What is meta learning
  - [[file:~/life/references/bibliography/pdfs/vilalta02_persp_view_survey_meta_learn.pdf][A Perspective View and Survey of Meta-Learning]]
  - [[file:~/life/references/bibliography/pdfs/finn17_model.pdf][MAML for deep learning]]
    - Multi-task learning
      - [[file:~/life/references/bibliography/pdfs/caruana97_multit_learn.pdf][Multi-Task learning]]
      - [[file:~/life/references/bibliography/pdfs/maurer13_spars.pdf][Sparse coding for multitask and transfer learning]]
      - [[file:~/life/references/bibliography/pdfs/ciliberto17_consis.pdf][Consistent Multitask learning with nonlinear output]]
      - [[file:~/life/references/bibliography/pdfs/ciliberto15_convex.pdf][Convex Learning of multiple tasks and their structure]]
      - [[file:~/life/references/bibliography/pdfs/ruder17_overv_multi_task_learn_deep_neural_networ.pdf][An Overview of Multi-Task Learning in Deep NNs]]
      - [[file:~/life/references/bibliography/pdfs/argyriou07_multi.pdf][Multi-Task Feature learning]]
      - [[file:~/life/references/bibliography/pdfs/zhang17_survey_multi_task_learn.pdf][A survey on Multi-Task Learning]]
    - Transfer learning
      - [[file:~/life/references/bibliography/pdfs/pan09_survey_trans_learn.pdf][A survey on Transfer Learning]]
      - [[file:~/life/references/bibliography/pdfs/raina07_self.pdf][Self-taught learning: Transfer Learning from unlabeled data]]
      - [[file:~/life/references/bibliography/pdfs/pan08_trans.pdf][Transfer learning via Dimensionality Reduction]]
  - Neigbouring fields
    - [[file:~/life/references/bibliography/pdfs/quionero-candela09_datas.pdf][Dataset Shift]]
    - Learning-to-learn
      - [[file:~/life/references/bibliography/pdfs/denevi18_increm_learn_to_learn_with_statis_guaran.pdf][Incremental Learning to Learn with statistical guarantees]]
      - [[file:~/life/references/bibliography/pdfs/denevi18_learn.pdf][Learning to learn around a common mean]]
    - Lifelong learning
    - Curriculum learning
- How can it be formulated
  - SLT
  - Other
- Equivalent problems in other fields
  - Statistics
  - Control
  - ML
  - Signal Processing
  - Operations Research
  - etc.
- Current approaches to Active Meta-Learning

* NEXT Current Meta Learning setup
:LOGBOOK:
- State "NEXT"       from              [2019-10-15 Tue 17:23]
:END:
Me and Carlo has brain spawned a bit and thought about ways to go about this. We
will attack this from a SLT point of view.

** Setup
We have a meta distribution \(\rho_{\mu}\) over some space of distributions
\(\D_{\Zc}\) such that \(\rho_{\Zc} \sim \rho_{\mu}\), where \(\Zc\) is our
sample space. Normally in supervised learning we have that \(\Zc = \X \times
\Y\). For each sampled distribution \(\rho_{\Zc}\) we sample a dataset (which we
will call /task/ to stay consistent with literature) \(\task =
(z_{i})_{i=1}^{n} \sim
\rho_{\Zc}^{n}\) iid, which is split up into \(n^{tr}, n^{val}\) sized dataset such
that \(\task = D^{tr} \cup D^{val} = (z_{i})_{i=1}^{n^{tr}} \cup
(z_{j})_{j=n^{tr}+1}^{n^{tr} + n^{val}}\) where \(D^{tr}, D^{val}\) is the train
and validation dataset respectively.

Now assume the following problem, we have \(m\) distributions
\((\rho_{i})_{i=1}^{m} \sim \rho_{\mu}^m\) sampled iid. Each of these
distributions \(\rho_{i}\) gives rise to a task \(\task_{i} = D_{i}^{tr} \cup
D_{i}^{val}\), where we make the simplifying assumption that \(n_{i} =
n^{tr}_{i} + n^{val}_{i}\) is the same for any \(i\), so that \(n^{tr}_{i} =
n^{tr}, n^{val}_{i} = n^{val}\) and \(n_{i} = n^{tr} + n^{val}\).

Our problem looks as follows, we want to find an algorithm
\(\malgo{\varphi}{\cdot}: \Zc^{\ast} \to \Hc\) where \(\Zc^{\ast}\) is the set of
all possible datasets of any size and \(\Hc\) is our hypothesis space, we let
\(\varphi\) denote the /hyperparameters/ (we will also write \(\varphi = \task_{I}\)
where \(I\) is some index set, e.g. \(I = \upto{k}\) to show that we have run
the meta-learning algorithm on \((\task_{1}, \dots, \task_{k})\)) of the algorithm which
we want to learn, as we are doing meta-learning rather than normal supervised
learning. In this sense we may learn \(\varphi\) from the outer loop, and the
algorithm then uses these to adjust its behaviour when mapping from the train
set to a target function in the hypothesis class. We assume that \(\Hc\) is the
same for all distributions, such that \(f_{i} \in \Hc\) for all \(i\), but
\(f_{i}\) differ in general.

(Rewriting of previous paragraph, we can view this as a three step process as
follows:
1. The hyper-meta algorithm takes sets of tasks \((\task_{i})_{i=1}^{m}\) and
   maps to algorithms that are
2. Meta algorithms that map from training sets \(D^{tr}\) to supervised
   algorithms that
3. Maps from training sets \(D^{tr}\) to a hypothesis set
Not sure if this is a fruitful way to look at it, but at least it clarifies the
different levels we are operating on here. One way to write this (sloppily)
could be as follows, if \(T^{\ast}\) is the set of all sequences of tasks, we
call \(\mathcal{M}_{1}\) the hyper-meta, \(\mathcal{M}_{0}\) the meta, and
\(\algo\) the base algorithm, then we have the following nested way of coupling
them
#+begin_export latex
\begin{equation*}
\mathcal{M}_1: T^{\ast} \to \{\mathcal{M}_0: \Zc^{\ast} \to \{ \algo: \Zc^{\ast} \to \Hc \} \}
\end{equation*}
#+end_export
)

We introduce the following notation, the loss function is a function
#+begin_export latex
\begin{equation*}
\ell: \Zc \times \Hc \to \R_{+}
\end{equation*}
#+end_export
which takes as input a sample point \(z \in \Zc\) and a hypothesis \(h \in \Hc\)
getting a loss of choosing this hypothesis for this sample point, \(\ell(h,
z)\). Note that this way of writing the loss may be highly non-linear as can be
seen in the following case when \(z = (x, y)\), \(\ell(z, h) = (h(x) - y)^{2}\).
The actual loss can now be written in the following form, for a task \(\task =
D^{tr} \cup D^{val}\), we have that the loss of the algorithm \(\malgo{\phi}{\cdot}\) is
#+begin_export latex
\begin{equation*}
L(\malgo{\phi}{\cdot}, \task) = \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \ell(\malgo{\phi}{D^{tr}}, z).
\end{equation*}
#+end_export

From this we can formulate the learning problem for meta learning. Given the
above, we want to understand how to perform well on the /meta-risk/
#+begin_export latex
\begin{equation*}
\err{\rho_{\mu}}{\malgo{\phi}{\cdot}} = \E_{\rho \sim \rho_{\mu}}\left[ \E_{\task \sim \rho^{n}} \left[L(\malgo{\phi}{\cdot}), \task) \right] \right].
\end{equation*}
#+end_export
It's clear that we can actually rewrite this further, since relying on the iid
assumption we have that
#+begin_export latex
\begin{align*}
  \E_{\task \sim \rho^{n}} \left[L(\malgo{\phi}{\cdot}), \task) \right] &= \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{D^{val} \sim \rho^{n_{val}}} \left[ \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \ell(\malgo{\phi}{D^{tr}}, z) \right] \right] \\
  & = \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{z \sim \rho} \left[ \ell(\malgo{\phi}{D^{tr}}, z) \right] \right]
\end{align*}
#+end_export
from which we get that the meta-risk can be expressed as
#+begin_export latex
\begin{align*}
  \err{\rho_{\mu}}{\malgo{\phi}{\cdot}} &= \E_{\rho \sim \rho_{\mu}} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \E_{z \sim \rho} \left[ \ell(\malgo{\phi}{D^{tr}}, z) \right] \right]  \right]\\
                                        &= \E_{\rho \sim \rho_{\mu}} \left[ \E_{D^{tr} \sim \rho^{n_{tr}}} \left[ \err{\rho}{\malgo{\phi}{D^{tr}}} \right] \right]
\end{align*}
#+end_export

A way to do this is to do well in probability over the train set which is what
is usually done in SLT. Explicitly, assuming as above that we have a set of distributions
(which we will call /base/ distributions, where base correspond to the base
level in contrast to /meta/ which corresponds to the meta level)
\((\rho_{i})_{i=1}^{m} \sim \rho_{\mu}^{m}\) iid, and each \(\rho_{i}\) gives
rise to a task \(\task_{i}\) sampled iid, then we are interested in bounds of
the form
#+begin_export latex
\begin{equation*}
\Pr_{(\task_i)_{i=1}^m}(\err{\rho_{\mu}}{\malgo{\phi}{\cdot}} - \err{\rho_{\mu}}{\algo_{\ast}} \geq \epsilon) \leq \delta,
\end{equation*}
#+end_export
where \(\algo_{\ast} = \inf_{\algo} \err{\rho_{\mu}}{\algo}\). We probably want
to constrain this in the future, but leave this like this for now.

** Approach to solving this
The general problem is hard to solve, instead we consider how the generalisation
error for an algorithm behaves. Consider the following expression (which
differs from the one above but taken from photos of what Carlo wrote on screen),
we assume that we have \(m\) different training tasks \(M = (\task_{i})_{i=1}^m\)
and will use the shorthand \(\task_{1:m}\) to mean all the tasks in index set.
For an active learning algorithm on a meta-level, for each \(t \leq m\) we let
\(M_{t}\) be a subset of tasks of size \(t\), \(M_{t} \subseteq M, \abs{M_{t}} =
t\). We are interested in quantifying the following
#+begin_export latex
\begin{equation*}
  \Pr_{M} \left( \E_{\task \sim \rho_T}[L(\malgo{M}{\cdot}, \task) - L(\malgo{M_t}{\cdot}, \task)] \geq \epsilon \right) \leq \delta
\end{equation*}
#+end_export

We make the following additional assumptions
- The base loss \(\ell(f(x), y)\) is Lipschitz with respect to the second argument with
  constant \(L\).
- The meta-algorithm \(\malgo{\phi}{\cdot}\) exists in some vector-valued
  reproducing kernel hilbert space cite:alvarez12_kernel_vector_valued_funct. In
  particular this means the following (following cite:ciliberto16), there is
  some vvRKHS \(\Gc\) consisting of functions mapping from \(\X \to \Hc\)
  where \(\Hc\) is some separable Hilbert space, we will assume that \(\Hc
  \subseteq \R^{d}\) since instances of datapoints normally comes in column
  form.

The definition of an vvRKHS is a generalisation of the univariate case. In
particular the vvRKHS \(\G\) is characterised by a so called /kernel of positive
type/ which is an operator values bi-linear map \(\Gamma: \X \times \X \to
B(\Hc, \Hc)\). Since we assume that \(\Hc\) is a subspace of Euclidean space,
\(\Gamma\) will map to positive semi-definite matrices. The vvRKHS is built in a
similar way to the univariate case with first a pre-Hilbert space which gets
completed by adding the limit points, with the inner product
#+begin_export latex
\begin{equation*}
\scal{\Gamma(x, \cdot))c}{\Gamma(x', \cdot)c'}_{\G} = \scal{\Gamma(x, x')c}{c'}_{\Hc}
\end{equation*}
#+end_export
which leads to the reproducing property, for any \(x \in \X, c \in \Hc\) and \(g
\in \G\), we have that
#+begin_export latex
\begin{equation*}
\scal{g(x)}{c}_{\Hc} = \scal{g}{\Gamma(x, \cdot)}{c}_{\G}
\end{equation*}
#+end_export
and that for each \(x \in \X\), the function \(\Gamma(x, \cdot): \G \to \Hc\) is
the evaluation function in \(x\) on \(\G\), that is \(\Gamma(x, \cdot)(g) =
g(x)\) and \(\Gamma(x, \cdot) \in \G\).
  
Consider now the expression in the expectation, the expected deviation of the
meta-loss between the meta-learning algorithm trained on the full dataset and
the subset of taska, we can write this as follows
#+begin_export latex
\begin{align*}
\abs{L(\malgo{M}{\task}) - L(\malgo{M_{t}}{\task})} &\leq \frac{1}{\abs{D^{val}}}\sum_{z \in D^{val}} \abs{\ell(\malgo{M}{D^{tr}}, z) - \ell(\malgo{M_{t}}{D^{tr}}, z)} \\
                                                    &= \frac{L}{\abs{D^{val}}}\sum_{x \in D^{val}} \abs{\malgo{M}{D^{tr}}(x) - \malgo{M_{t}}{D^{tr}}(x)} \\
\end{align*}
#+end_export


* bibliography
bibliography:../../../../bibliography/references.bib
bibliographystyle:unsrt
